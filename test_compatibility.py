#!/usr/bin/env python3
"""
å…¼å®¹æ€§æµ‹è¯•è„šæœ¬ - éªŒè¯å‡çº§åçš„æ‰€æœ‰åŠŸèƒ½
"""

import sys
from pathlib import Path

# æ·»åŠ  backend åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "backend"))

def test_imports():
    """æµ‹è¯•æ‰€æœ‰å…³é”®æ¨¡å—å¯¼å…¥"""
    print("=" * 60)
    print("1. æµ‹è¯•æ¨¡å—å¯¼å…¥")
    print("=" * 60)

    modules_to_test = [
        ("FastAPI app", "app"),
        ("Core config", "core.config"),
        ("Core database", "core.database"),
        ("Core trainer", "core.trainer"),
        ("Core model_manager", "core.model_manager"),
        ("Core data_processor", "core.data_processor"),
        ("Core evaluator", "core.evaluator"),
        ("Models schemas", "models.schemas"),
        ("API routes", "api"),
    ]

    failed = []
    for name, module_path in modules_to_test:
        try:
            __import__(module_path)
            print(f"âœ… {name}: OK")
        except Exception as e:
            print(f"âŒ {name}: FAILED - {str(e)}")
            failed.append((name, str(e)))

    print()
    return len(failed) == 0, failed


def test_transformers_api():
    """æµ‹è¯• Transformers API å…¼å®¹æ€§"""
    print("=" * 60)
    print("2. æµ‹è¯• Transformers API å…¼å®¹æ€§")
    print("=" * 60)

    try:
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            TrainingArguments,
            Trainer,
            BitsAndBytesConfig,
        )

        # æµ‹è¯• BitsAndBytesConfig
        import torch
        config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
        print("âœ… BitsAndBytesConfig: OK")

        # æµ‹è¯• TrainingArguments
        args = TrainingArguments(
            output_dir="./test_output",
            num_train_epochs=1,
            per_device_train_batch_size=1,
            save_steps=100,
        )
        print("âœ… TrainingArguments: OK")

        print("âœ… æ‰€æœ‰ Transformers API æµ‹è¯•é€šè¿‡")
        print()
        return True, []
    except Exception as e:
        print(f"âŒ Transformers API æµ‹è¯•å¤±è´¥: {str(e)}")
        print()
        return False, [("Transformers API", str(e))]


def test_peft_api():
    """æµ‹è¯• PEFT API å…¼å®¹æ€§"""
    print("=" * 60)
    print("3. æµ‹è¯• PEFT API å…¼å®¹æ€§")
    print("=" * 60)

    try:
        from peft import LoraConfig, get_peft_model

        # æµ‹è¯• LoraConfig
        lora_config = LoraConfig(
            r=64,
            lora_alpha=128,
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
            target_modules=["q_proj", "v_proj"],
        )
        print("âœ… LoraConfig: OK")
        print("âœ… PEFT API æµ‹è¯•é€šè¿‡")
        print()
        return True, []
    except Exception as e:
        print(f"âŒ PEFT API æµ‹è¯•å¤±è´¥: {str(e)}")
        print()
        return False, [("PEFT API", str(e))]


def test_pydantic_schemas():
    """æµ‹è¯• Pydantic Schemas"""
    print("=" * 60)
    print("4. æµ‹è¯• Pydantic Schemas")
    print("=" * 60)

    try:
        from models.schemas import (
            TrainingConfigCreate,
            TrainingConfigResponse,
            TrainingTaskResponse,
            ModelInfo,
            DownloadModelRequest,
        )

        # æµ‹è¯• ModelInfo (æœ‰ model_name å’Œ model_size å­—æ®µ)
        model_info = ModelInfo(
            model_name="Qwen/Qwen3-4B",
            model_size="4B",
            parameters=4000000000,
            max_seq_length=32768,
            description="Test model",
            supported_training_methods=["lora", "qlora"],
            recommended=True,
        )
        print(f"âœ… ModelInfo: OK (model_name={model_info.model_name})")

        # æµ‹è¯• TrainingConfigCreate
        config = TrainingConfigCreate(
            name="Test Config",
            model_name="Qwen/Qwen3-4B",
            training_method="lora",
        )
        print(f"âœ… TrainingConfigCreate: OK")

        # æµ‹è¯• DownloadModelRequest
        download_req = DownloadModelRequest(
            model_name="Qwen/Qwen3-4B",
            force_download=False,
        )
        print(f"âœ… DownloadModelRequest: OK")

        print("âœ… æ‰€æœ‰ Pydantic schemas æµ‹è¯•é€šè¿‡")
        print()
        return True, []
    except Exception as e:
        print(f"âŒ Pydantic schemas æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        print()
        return False, [("Pydantic schemas", str(e))]


def test_trainer_config():
    """æµ‹è¯• Trainer é…ç½®"""
    print("=" * 60)
    print("5. æµ‹è¯• Trainer é…ç½®")
    print("=" * 60)

    try:
        from core.trainer import TrainingConfig, Trainer_Qwen3

        # åˆ›å»ºè®­ç»ƒé…ç½®
        config = TrainingConfig(
            model_name="Qwen/Qwen3-4B",
            training_method="lora",
            device="cpu",  # ä½¿ç”¨ CPU é¿å… GPU ä¾èµ–
            num_train_epochs=1,
            load_in_4bit=False,  # CPU ä¸æ”¯æŒé‡åŒ–
        )
        print("âœ… TrainingConfig: OK")

        # åˆå§‹åŒ– Trainerï¼ˆä¸åŠ è½½æ¨¡å‹ï¼‰
        trainer = Trainer_Qwen3(config)
        print(f"âœ… Trainer_Qwen3: OK (device={trainer.device})")

        print("âœ… Trainer é…ç½®æµ‹è¯•é€šè¿‡")
        print()
        return True, []
    except Exception as e:
        print(f"âŒ Trainer é…ç½®æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        print()
        return False, [("Trainer config", str(e))]


def test_model_manager():
    """æµ‹è¯• Model Manager"""
    print("=" * 60)
    print("6. æµ‹è¯• Model Manager")
    print("=" * 60)

    try:
        from core.model_manager import get_model_manager

        manager = get_model_manager()
        print(f"âœ… ModelManager åˆå§‹åŒ–: OK")
        print(f"   ç¼“å­˜ç›®å½•: {manager.cache_dir}")
        print(f"   ä½¿ç”¨ ModelScope: {manager.use_modelscope}")

        # æµ‹è¯•åˆ—å‡ºç¼“å­˜æ¨¡å‹
        cached = manager.list_cached_models()
        print(f"âœ… åˆ—å‡ºç¼“å­˜æ¨¡å‹: OK ({len(cached)} ä¸ªæ¨¡å‹)")

        print("âœ… Model Manager æµ‹è¯•é€šè¿‡")
        print()
        return True, []
    except Exception as e:
        print(f"âŒ Model Manager æµ‹è¯•å¤±è´¥: {str(e)}")
        print()
        return False, [("Model Manager", str(e))]


def test_device_detection():
    """æµ‹è¯•è®¾å¤‡æ£€æµ‹"""
    print("=" * 60)
    print("7. æµ‹è¯•è®¾å¤‡æ£€æµ‹")
    print("=" * 60)

    try:
        from core.devices import resolve_device, coerce_torch_dtype
        import torch

        device = resolve_device("auto")
        print(f"âœ… è‡ªåŠ¨è®¾å¤‡æ£€æµ‹: {device}")

        dtype = coerce_torch_dtype(device)
        dtype_str = str(dtype).replace("torch.", "") if dtype else "None"
        print(f"âœ… dtype æ¨æ–­: {dtype_str}")

        print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… CUDA å¯ç”¨: {torch.cuda.is_available()}")
        print(f"âœ… MPS å¯ç”¨: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False}")

        print("âœ… è®¾å¤‡æ£€æµ‹æµ‹è¯•é€šè¿‡")
        print()
        return True, []
    except Exception as e:
        print(f"âŒ è®¾å¤‡æ£€æµ‹æµ‹è¯•å¤±è´¥: {str(e)}")
        print()
        return False, [("Device detection", str(e))]


def print_version_info():
    """æ‰“å°ç‰ˆæœ¬ä¿¡æ¯"""
    print("=" * 60)
    print("ä¾èµ–ç‰ˆæœ¬ä¿¡æ¯")
    print("=" * 60)

    try:
        import torch
        print(f"PyTorch: {torch.__version__}")
    except:
        print("PyTorch: æœªå®‰è£…")

    try:
        import transformers
        print(f"Transformers: {transformers.__version__}")
    except:
        print("Transformers: æœªå®‰è£…")

    try:
        import peft
        print(f"PEFT: {peft.__version__}")
    except:
        print("PEFT: æœªå®‰è£…")

    try:
        import accelerate
        print(f"Accelerate: {accelerate.__version__}")
    except:
        print("Accelerate: æœªå®‰è£…")

    try:
        import bitsandbytes
        print(f"BitsAndBytes: {bitsandbytes.__version__}")
    except:
        print("BitsAndBytes: æœªå®‰è£…")

    try:
        import pydantic
        print(f"Pydantic: {pydantic.__version__}")
    except:
        print("Pydantic: æœªå®‰è£…")

    try:
        import fastapi
        print(f"FastAPI: {fastapi.__version__}")
    except:
        print("FastAPI: æœªå®‰è£…")

    print()


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\nğŸš€ å¼€å§‹å…¼å®¹æ€§æµ‹è¯•...\n")

    print_version_info()

    results = []
    all_errors = []

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        test_imports,
        test_transformers_api,
        test_peft_api,
        test_pydantic_schemas,
        test_trainer_config,
        test_model_manager,
        test_device_detection,
    ]

    for test_func in tests:
        success, errors = test_func()
        results.append(success)
        all_errors.extend(errors)

    # æ‰“å°æ€»ç»“
    print("=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)

    passed = sum(results)
    total = len(results)

    print(f"é€šè¿‡: {passed}/{total}")
    print(f"å¤±è´¥: {total - passed}/{total}")

    if all_errors:
        print("\nå¤±è´¥çš„æµ‹è¯•:")
        for name, error in all_errors:
            print(f"  âŒ {name}: {error}")

    print()

    if all(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å‡çº§åçš„ä»£ç å®Œå…¨å…¼å®¹ã€‚")
        return 0
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(main())
