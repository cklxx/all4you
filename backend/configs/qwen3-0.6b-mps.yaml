# Qwen3-0.6B LoRA configuration optimized for Apple Silicon (MPS)

# Model Configuration
model_name: "Qwen/Qwen3-0.6B"
model_type: "causal"
device: "mps"
torch_dtype: "float16"

# Training Method
training_method: "lora"

# Training Arguments
num_train_epochs: 3
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2e-4
max_grad_norm: 1.0
warmup_ratio: 0.1
weight_decay: 0.01

# Optimizer and LR Scheduler
optim: "adamw_torch"
lr_scheduler_type: "cosine"
max_steps: -1

# Data Configuration
max_seq_length: 2048
seed: 42

# Model Quantization
load_in_4bit: false
load_in_8bit: false

# LoRA Configuration
lora_rank: 32
lora_alpha: 64
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "v_proj"

# Logging and Checkpointing
logging_steps: 10
eval_steps: 50
save_steps: 100
evaluation_strategy: "steps"
save_strategy: "steps"
save_total_limit: 3

# Hardware Configuration
use_flash_attention: false
gradient_checkpointing: true
fp16: true
bf16: false

# Output Directory
output_dir: "./outputs"
